{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from milnlp.converters.pdf_to_text import PdfConverter, create_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filepath = \"pdf_examples/normal_document.pdf\"\n",
    "# filepath = \"pdf_examples/powerpoint.pdf\"\n",
    "# filepath = \"pdf_examples/glossary.pdf\" \n",
    "filepath = \"pdf_examples/challenge.pdf\"\n",
    "\n",
    "converter = PdfConverter()\n",
    "data = converter.convert_pdf(filepath, to_file=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1 - Break sentences on \\n\\n and remove \\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "pattern = re.compile(r'\\D([!?.]|[!?.][\"”])[ \\n]')  # pattern for separating sentences\n",
    "\n",
    "raw_paragraphs = data.split('\\n\\n')\n",
    "# Remove PDF newline artifacts\n",
    "document = [None]*len(raw_paragraphs)\n",
    "cleaned_paragraphs = [None]*len(raw_paragraphs)\n",
    "for ii, raw_paragraph in enumerate(raw_paragraphs):\n",
    "    cleaned_paragraphs[ii] = raw_paragraph.replace(\"\\n\", \"\")\n",
    "    \n",
    "    # Split up normal structured sentences based on the idea of punctuation followed by a space or newline. \n",
    "    query = pattern.finditer(cleaned_paragraphs[ii])\n",
    "    \n",
    "    start = 0\n",
    "    sentences = []\n",
    "    for match in query:\n",
    "        end, next_start = match.span()\n",
    "        raw_sentence = cleaned_paragraphs[ii][start:end]+match[0][:-1]  # up to index -1 because the match could be '. ' or '.\" '\n",
    "        # Clean up common sentence issues\n",
    "        sentence = raw_sentence.strip(' ')  # extra leading/trailing spaces\n",
    "        # Todo migrate utilities from codebase to simple implementations here\n",
    "        # remove weblinks\n",
    "        # remove headings (maybe) re =~ (\\d+\\w?[.):-])+ +  Link: regexr.com/41pqg\n",
    "        # remove short lines (maybe)\n",
    "        # remove unlikely sentences (not enough words or something)\n",
    "        # remove non-ascii (i.e. unicode and hex which can happen in weird cases)\n",
    "        \n",
    "        # Add sentence to paragraph\n",
    "        sentences.append(sentence)  # append the punctuation so that the sentence is still complete\n",
    "        start = next_start\n",
    "        \n",
    "    # Do something with sentences todo improve\n",
    "    document[ii] = sentences\n",
    "    \n",
    "# Remove empty paragraphs from document  todo later we may want to use multiple newlines (adjacent empty paragraphs) to infer natural sections of text\n",
    "document = [paragraph for paragraph in document if paragraph] "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Missing sentences glossary example\n",
    "for sentence in document[41]:\n",
    "    print(sentence, '\\n')\n",
    "    \n",
    "for sentence in document[42]:\n",
    "    print(sentence, '\\n')\n",
    "    \n",
    "for sentence in document[43]:\n",
    "    print(sentence, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Operation  of  multi-crew  aircraft  with  variably  reduced  onboard  crew  is  a  considerable challenge. \n",
      "\n",
      "Multiple crew positions are generally implemented when the operating workload exceeds the reasonable capability of a single operator or when mission safety considerations mandate  the  redundancy  of  additional  crew  members. \n",
      "\n",
      "Interactions  of  skilled  multi-crew operator teams can be highly complex. \n",
      "\n",
      "At the same time, there is considerable potential payoff to robustly achieving the vision of variably crewed operations, including reduced training and operating  costs,  and  freeing  aircraft  operators  from  low  level  tasks  to  enhance  their  overall mission capability. \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for sentence in document[7]:\n",
    "    print(sentence, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['• SOO Req.'],\n",
       " ['SOO Req.', '1SOO Req.', '2SOO Req.', '3SOO Req.'],\n",
       " ['integration services for the work.']]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4139"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Powerpoint files are almost always aready summaries of topics, and the information they convey is often inadequate without the context of a \"slide\" or heading from the section of a slide. Thus, it does not seem practical or feasible to extract out sentences from the powerpoint and attempt to score their value without the reamining slide context. It is also not feasible to implement a rapid solution to extract out chunks of text from slides based on their context due to formatting issues with PDF. The task of using information within PPT files should be delegated to future efforts to parse the PPT file directly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "string index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-167-88e23d06d556>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mparagraph\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mparagraph\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mparagraph\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcleaned_paragraphs\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mparagraph\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0mparagraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mparagraph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[0mparagraph\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;34m'-'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'•'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mflagged\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: string index out of range"
     ]
    }
   ],
   "source": [
    "# Test for PPT\n",
    "threshhold = 0.3\n",
    "\n",
    "flagged = 0\n",
    "for paragraph in cleaned_paragraphs:  # breaks if paragraph is empty\n",
    "    paragraph = paragraph.strip(' ')\n",
    "    if paragraph[0] in {'-', '•', ''}:\n",
    "        flagged += 1\n",
    "\n",
    "print(f\"{flagged}/{len(cleaned_paragraphs)} flagged as presentation elements\")\n",
    "if flagged/len(cleaned_paragraphs) > threshhold:\n",
    "    print(\"Skipping file due to lack of substantial natural language text.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ecct]",
   "language": "python",
   "name": "conda-env-ecct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
