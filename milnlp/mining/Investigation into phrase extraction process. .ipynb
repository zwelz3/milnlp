{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Normally, the extract_candidate_words function is only used on pre-processed text, not raw text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "\n",
    "from milnlp.converters.pdf_to_text import PdfConverter\n",
    "from milnlp.converters.text_utils import process_raw_into_lines\n",
    "from milnlp.converters.pdf_to_text import create_sumy_dom\n",
    "from milnlp.mining.phrases import extract_candidate_words, score_keyphrases_by_textrank"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert pdf to raw text\n",
    "converter = PdfConverter()\n",
    "text = converter.convert_pdf(\n",
    "    r\"C:\\Users\\zwelz3\\Documents\\GTRI_Projects\\ECCT_EW_EMS\\Market Research\\Concepts, Processes, Approaches\\Aircraft Design\\Materials\\2) Graphene Meta-Material Absorber.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert raw text to pre-processed text\n",
    "document_text = process_raw_into_lines(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert pre-processed text into a sumy document object\n",
    "token = Tokenizer('english')\n",
    "document = create_sumy_dom(document_text, token)\n",
    "doc_text = ' '.join([sentence._text for sentence in document.sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract candidates from document text\n",
    "candidates = extract_candidate_words(doc_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "graphene nanoribbon not found\n",
      "graphene nanoribbons not found\n"
     ]
    }
   ],
   "source": [
    "# no multi-word candidates, these are created in the scoring algorithm.\n",
    "for poi in {\"nanoribbon\",\"nanoribbons\",\"graphene nanoribbon\",\"graphene nanoribbons\"}:\n",
    "    try:\n",
    "        assert poi in candidates\n",
    "    except AssertionError:\n",
    "        print(poi, \"not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The POI showed up 17 times even though it was not selected as a candidate.\n"
     ]
    }
   ],
   "source": [
    "sis = []\n",
    "for si, sentence in enumerate(document.sentences):\n",
    "    #print(sentence._text, '\\n')\n",
    "    if \"graphene nanoribbon\" in sentence._text:\n",
    "        sis.append(si)\n",
    "        \n",
    "print(f\"The POI showed up {len(sis)} times even though it was not selected as a candidate.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deconstruct the extraction process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools, nltk, string\n",
    "from itertools import takewhile, tee\n",
    "import networkx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct = set(string.punctuation)\n",
    "stop_words = set(nltk.corpus.stopwords.words('english'))\n",
    "# tokenize and POS-tag words\n",
    "tagged_sentences = nltk.pos_tag_sents(nltk.word_tokenize(sent) for sent in nltk.sent_tokenize(text))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### looking at the following two cells, it looks like superscripts/subscripts end up smashed into the word they are next to, which causes nasty candidates to pass through. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mode of the nanodisks together with multi-reflection from the assistants of total internal reflection and \\nmetal reflection can result in a complete optical absorption28.'"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for ss, sent in enumerate(nltk.sent_tokenize(text)):\n",
    "    if ss == 25:\n",
    "        break\n",
    "\n",
    "sent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('mode', 'NN'),\n",
       " ('of', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('nanodisks', 'NNS'),\n",
       " ('together', 'RB'),\n",
       " ('with', 'IN'),\n",
       " ('multi-reflection', 'NN'),\n",
       " ('from', 'IN'),\n",
       " ('the', 'DT'),\n",
       " ('assistants', 'NNS'),\n",
       " ('of', 'IN'),\n",
       " ('total', 'JJ'),\n",
       " ('internal', 'JJ'),\n",
       " ('reflection', 'NN'),\n",
       " ('and', 'CC'),\n",
       " ('metal', 'JJ'),\n",
       " ('reflection', 'NN'),\n",
       " ('can', 'MD'),\n",
       " ('result', 'VB'),\n",
       " ('in', 'IN'),\n",
       " ('a', 'DT'),\n",
       " ('complete', 'JJ'),\n",
       " ('optical', 'JJ'),\n",
       " ('absorption28', 'NN'),\n",
       " ('.', '.')]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tagged_sentences[25]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 7, 31, 36, 37, 40, 45, 46, 49, 61, 64, 84, 85, 90, 94, 95, 99]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There are also a significant number of broken, single letter candidates that should be removed. Better pre-processing should alleviate these cases, but additional removal may be needed from the raw candidates. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing how nltk tokenizer tags POS for sentence with broken elements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "A big dog jumped.\n",
    "A big A dog jumped.\n",
    "A big a dog jumped. \n",
    "A big dog A jumped.\n",
    "A big dog a jumped.\n",
    "A dog A jumped. \n",
    "A dog a jumped. \n",
    "\"\"\"\n",
    "tok_sent = nltk.pos_tag_sents(nltk.word_tokenize(sent) for ss, sent in enumerate(nltk.sent_tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. [('A', 'DT'), ('big', 'JJ'), ('dog', 'NN'), ('jumped', 'VBD'), ('.', '.')]\n",
      "1. [('A', 'DT'), ('big', 'JJ'), ('A', 'NN'), ('dog', 'NN'), ('jumped', 'NN'), ('.', '.')]\n",
      "2. [('A', 'DT'), ('big', 'JJ'), ('a', 'DT'), ('dog', 'NN'), ('jumped', 'NN'), ('.', '.')]\n",
      "3. [('A', 'DT'), ('big', 'JJ'), ('dog', 'NN'), ('A', 'DT'), ('jumped', 'NN'), ('.', '.')]\n",
      "4. [('A', 'DT'), ('big', 'JJ'), ('dog', 'NN'), ('a', 'DT'), ('jumped', 'NN'), ('.', '.')]\n",
      "5. [('A', 'DT'), ('dog', 'NN'), ('A', 'NNP'), ('jumped', 'NN'), ('.', '.')]\n",
      "6. [('A', 'DT'), ('dog', 'NN'), ('a', 'DT'), ('jumped', 'NN'), ('.', '.')]\n"
     ]
    }
   ],
   "source": [
    "for ii, sent in enumerate(tok_sent):\n",
    "    print(f\"{ii}.\", sent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note the different behavior depending on where the broken text shows up and the case of the text. \n",
    "0. normal sentence\n",
    "1. UC injected between JJ and NN -> NN\n",
    "2. LC injected between JJ and NN -> DT\n",
    "3. UC injected between JJ+NN and VBD -> DT (VBD now NN)\n",
    "4. LC injected between JJ+NN and VBD -> DT (VBD now NN)\n",
    "5. UC injected between NN and VBD -> NNP (VBD now NN)\n",
    "6. LC injected between NN and VBD -> DT (VBD now NN)\n",
    "\n",
    "Especially note cases 3-6 where there is now no verb in the sentence because of the corrupt text. This causes issues later on when TextRank attempts to score sentences. \n",
    "\n",
    "*Food-for-thought. If it gets this messed up with a single broken character, the behavior is unpredictable for n-number of broken characters and even worse for multiple broken characters in adjacent sentences that are also broken (i.e. assuming the sentence tokenizer fails).*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing additional sentences to see how they get tokenized and tagged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"\n",
    "Yuancheng Fan1, Zhe Liu2, Fuli Zhang1, Qian Zhao3, Zeyong Wei4, Quanhong Fu1, Junjie Li2,  Changzhi Gu2 & Hongqiang Li4\n",
    "\n",
    "The graphene layer is considered as a sheet material modeled with complex surface conductivity (σg) \n",
    "since a one-atom-thick graphene sheet is sufficiently thin compared with the concerned wavelength. In \n",
    "the theoretical perspective based on random-phase-approximation (RPA)54–56, the complex conductivity \n",
    "1 , especially in heavily \n",
    ")−\n",
    "of graphene can be described by the Drude model as \n",
    "doped  region  and  low  frequencies  (far  below  Fermi  energy),  where  EF  represents  the  Fermi  energy, \n",
    "2 is the relaxation rate with the mobility μ =  104 cm2V−1s−1 and Fermi velocity vF ≈  106 m/s.\n",
    "µ\n",
    "=\n",
    "τ\n",
    "F\n",
    "\n",
    "The  complex  scattering  coefficients  (O±)  of  the  graphene  nanoribbon  meta-surface  can  be  related \n",
    "to the two input beams (I±, and in this paper the two input beams are set to be of equal amplitude I) \n",
    "through a scattering matrix, Sg, defined as:\n",
    "\n",
    "\"\"\"\n",
    "tok_sent = nltk.pos_tag_sents(nltk.word_tokenize(sent) for ss, sent in enumerate(nltk.sent_tokenize(text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. [('Yuancheng', 'NNP'), ('Fan1', 'NNP'), (',', ','), ('Zhe', 'NNP'), ('Liu2', 'NNP'), (',', ','), ('Fuli', 'NNP'), ('Zhang1', 'NNP'), (',', ','), ('Qian', 'NNP'), ('Zhao3', 'NNP'), (',', ','), ('Zeyong', 'NNP'), ('Wei4', 'NNP'), (',', ','), ('Quanhong', 'NNP'), ('Fu1', 'NNP'), (',', ','), ('Junjie', 'NNP'), ('Li2', 'NNP'), (',', ','), ('Changzhi', 'NNP'), ('Gu2', 'NNP'), ('&', 'CC'), ('Hongqiang', 'NNP'), ('Li4', 'NNP'), ('The', 'DT'), ('graphene', 'NN'), ('layer', 'NN'), ('is', 'VBZ'), ('considered', 'VBN'), ('as', 'IN'), ('a', 'DT'), ('sheet', 'NN'), ('material', 'NN'), ('modeled', 'VBN'), ('with', 'IN'), ('complex', 'JJ'), ('surface', 'NN'), ('conductivity', 'NN'), ('(', '('), ('σg', 'NN'), (')', ')'), ('since', 'IN'), ('a', 'DT'), ('one-atom-thick', 'JJ'), ('graphene', 'NN'), ('sheet', 'NN'), ('is', 'VBZ'), ('sufficiently', 'RB'), ('thin', 'JJ'), ('compared', 'VBN'), ('with', 'IN'), ('the', 'DT'), ('concerned', 'JJ'), ('wavelength', 'NN'), ('.', '.')] \n",
      "\n",
      "1. [('In', 'IN'), ('the', 'DT'), ('theoretical', 'JJ'), ('perspective', 'NN'), ('based', 'VBN'), ('on', 'IN'), ('random-phase-approximation', 'NN'), ('(', '('), ('RPA', 'NNP'), (')', ')'), ('54–56', 'CD'), (',', ','), ('the', 'DT'), ('complex', 'JJ'), ('conductivity', 'NN'), ('1', 'CD'), (',', ','), ('especially', 'RB'), ('in', 'IN'), ('heavily', 'RB'), (')', ')'), ('−', 'NN'), ('of', 'IN'), ('graphene', 'NN'), ('can', 'MD'), ('be', 'VB'), ('described', 'VBN'), ('by', 'IN'), ('the', 'DT'), ('Drude', 'NNP'), ('model', 'NN'), ('as', 'IN'), ('doped', 'JJ'), ('region', 'NN'), ('and', 'CC'), ('low', 'JJ'), ('frequencies', 'NNS'), ('(', '('), ('far', 'RB'), ('below', 'IN'), ('Fermi', 'NNP'), ('energy', 'NN'), (')', ')'), (',', ','), ('where', 'WRB'), ('EF', 'NNP'), ('represents', 'VBZ'), ('the', 'DT'), ('Fermi', 'NNP'), ('energy', 'NN'), (',', ','), ('2', 'CD'), ('is', 'VBZ'), ('the', 'DT'), ('relaxation', 'NN'), ('rate', 'NN'), ('with', 'IN'), ('the', 'DT'), ('mobility', 'NN'), ('μ', 'NNP'), ('=', 'VBZ'), ('104', 'CD'), ('cm2V−1s−1', 'NN'), ('and', 'CC'), ('Fermi', 'NNP'), ('velocity', 'NN'), ('vF', 'NN'), ('≈', 'VBD'), ('106', 'CD'), ('m/s', 'NN'), ('.', '.')] \n",
      "\n",
      "2. [('µ', 'JJ'), ('=', 'NNP'), ('τ', 'NNP'), ('F', 'NNP'), ('The', 'DT'), ('complex', 'JJ'), ('scattering', 'NN'), ('coefficients', 'NNS'), ('(', '('), ('O±', 'NNP'), (')', ')'), ('of', 'IN'), ('the', 'DT'), ('graphene', 'NN'), ('nanoribbon', 'IN'), ('meta-surface', 'NN'), ('can', 'MD'), ('be', 'VB'), ('related', 'VBN'), ('to', 'TO'), ('the', 'DT'), ('two', 'CD'), ('input', 'NN'), ('beams', 'NNS'), ('(', '('), ('I±', 'NNP'), (',', ','), ('and', 'CC'), ('in', 'IN'), ('this', 'DT'), ('paper', 'NN'), ('the', 'DT'), ('two', 'CD'), ('input', 'NN'), ('beams', 'NNS'), ('are', 'VBP'), ('set', 'VBN'), ('to', 'TO'), ('be', 'VB'), ('of', 'IN'), ('equal', 'JJ'), ('amplitude', 'NN'), ('I', 'PRP'), (')', ')'), ('through', 'IN'), ('a', 'DT'), ('scattering', 'NN'), ('matrix', 'NN'), (',', ','), ('Sg', 'NNP'), (',', ','), ('defined', 'VBD'), ('as', 'IN'), (':', ':')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ii, sent in enumerate(tok_sent):\n",
    "    print(f\"{ii}.\", sent, '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0. [('µ', 'JJ'), ('=', 'NNP'), ('τ^34', 'NNP'), ('+', 'VBD'), ('10', 'CD')] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "text = \"µ = τ^34 + 10\"\n",
    "tok_sent = nltk.pos_tag_sents(nltk.word_tokenize(sent) for ss, sent in enumerate(nltk.sent_tokenize(text)))\n",
    "for ii, sent in enumerate(tok_sent):\n",
    "    print(f\"{ii}.\", sent, '\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notes:\n",
    "- because the sentence tokenizer cannot divide sentences properly, the POS tagger gets confused and improperly tags\n",
    "\n",
    "- first, we must pre-process the text to ensure that sentences are tokenized correctly (different branch).\n",
    "  > then merge into this branch\n",
    "- second, we need to remove sentences that do not meet some criteria (i.e. there is not enough normal language structure)\n",
    "  > this would get rid of sentences that are created from: labels (figure, section), lists (bullets, pptx), references, glossary/index, author lists, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ecct]",
   "language": "python",
   "name": "conda-env-ecct-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
